[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "fedge-simulation"
version = "1.0.0"
description = "Hierarchical Federated Learning - In-Memory Simulation Mode for CIFAR-10"
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]>=1.18.0",
    "torch>=2.1",
    "torchvision>=0.15",
    "scikit-learn>=1.4.2",
    "pandas>=2.2.0",
    "filelock>=3.13.0",
    "psutil>=5.9.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "numpy>=1.24.0",
    "scipy>=1.10.0",
    "toml>=0.10.2",
    "pillow>=9.0.0"
]

[tool.hatch.build.targets.wheel]
packages = ["."]

# Core Hierarchical FL Configuration
[tool.flwr.hierarchy]
num_servers = 3
clients_per_server = [5, 5, 5]
global_rounds = 200
server_rounds_per_global = 1
# Note: cloud_port and server_base_port removed (not needed for in-memory simulation)
weight_decay = 0.0005            # v12/v13: L2 regularization
clip_norm = 1.0
momentum = 0.9
lr_gamma = 0.995                 # LR decay: ~0.37 of initial LR by round 200
prox_mu = 0.0                    # v3: Disabled (pure FedAvg + clustering)

# Dataset
dataset_flag = "cifar10"

# v7: Global averaging (v3 architecture)
server_isolation = false         # All servers get same global model

# Client-level SCAFFOLD (v9: fixed amplification and clipping bugs)
# v16: Safety clip threshold 2.0 -> 3.0 (v15 achieved 66.5%, target 68-70%)
# v15: Added safety clip AFTER SCAFFOLD (v14 collapsed due to unclipped corrections)
# v14: REVERSED alpha values (client non-IID, server IID) - SCAFFOLD now corrects real drift
# v13 bug fixes in task.py: removed double LR decay, clip before SCAFFOLD, dtype/device fix
scaffold_enabled = true          # Enable SCAFFOLD from round 1
scaffold_scaling_factor = 0.1    # v9/v13: proven stable value (v11's 1.0 failed)
scaffold_correction_clip = 0.1   # v9: tight bound on corrections applied to gradients
scaffold_warmup_rounds = 10      # v9: gradual activation to prevent sudden collapse
scaffold_clip_value = 1.0        # v9: tightened from 10.0 to prevent control variate explosion

# Server-level SCAFFOLD (disabled - not needed with global averaging)
scaffold_server_enabled = false
scaffold_server_lr = 1.0
scaffold_correction_lr = 0.1

# Learning parameters
lr_init = 0.01
server_lr = 1.0
global_lr = 1.0
local_epochs = 5

# Accuracy gate (0.0 = accept all improvements)
cluster_better_delta = 0.0

# Evaluation
eval_batch_size = 64

# Dirichlet Partitioning - v14: REVERSED to match standard FL benchmarks
[tool.flwr.hierarchy.dirichlet]
alpha_server = 1000.0 # v14: IID across servers (was 0.5) - servers have similar data
alpha_client = 0.5    # v14: Non-IID within servers (was 1000) - SCAFFOLD can correct real drift
seed         = 42


# Cloud Clustering Configuration
[tool.flwr.cloud_cluster]
enable = true
start_round = 30                 # clustering begins after models differentiate
frequency = 1                    # cluster every cloud round
method = "weight"                # "weight" clustering (v3 approach)
tau = 0.4                        # Similarity threshold (v3 value)

# System Configuration
[tool.flwr.cluster.batch_sizes]
logit_batch = 256
feature_batch = 256

[tool.flwr.system]
max_workers = 4
log_level = "INFO"
verbose_logging = true
# Note: gc_frequency hardcoded in orchestrator (every 10 rounds) like HHAR
# Note: communication_timeout, max_retries removed (not needed for in-memory)
