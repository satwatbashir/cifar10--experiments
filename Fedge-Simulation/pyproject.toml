[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "fedge-simulation"
version = "1.0.0"
description = "Hierarchical Federated Learning - In-Memory Simulation Mode for CIFAR-10"
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]>=1.18.0",
    "torch>=2.1",
    "torchvision>=0.15",
    "scikit-learn>=1.4.2",
    "pandas>=2.2.0",
    "filelock>=3.13.0",
    "psutil>=5.9.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "numpy>=1.24.0",
    "scipy>=1.10.0",
    "toml>=0.10.2",
    "pillow>=9.0.0"
]

[tool.hatch.build.targets.wheel]
packages = ["."]

# Core Hierarchical FL Configuration
[tool.flwr.hierarchy]
num_servers = 3
clients_per_server = [5, 5, 5]
global_rounds = 200
server_rounds_per_global = 1
# Note: cloud_port and server_base_port removed (not needed for in-memory simulation)
weight_decay = 0.0
clip_norm = 1.0
momentum = 0.9
lr_gamma = 0.995                 # LR decay: ~0.37 of initial LR by round 200
prox_mu = 0.0                    # v3: Disabled (pure FedAvg + clustering)

# Dataset
dataset_flag = "cifar10"

# v7: Global averaging (v3 architecture)
server_isolation = false         # All servers get same global model

# Client-level SCAFFOLD (v8: enabled from round 1)
scaffold_enabled = true          # v8: SCAFFOLD from start, no warmup delay

# Server-level SCAFFOLD (disabled for v7 - not needed with global averaging)
scaffold_server_enabled = false
scaffold_server_lr = 1.0
scaffold_correction_lr = 0.1
scaffold_clip_value = 10.0

# Learning parameters
lr_init = 0.01
server_lr = 1.0
global_lr = 1.0
local_epochs = 5

# Accuracy gate (0.0 = accept all improvements)
cluster_better_delta = 0.0

# Evaluation
eval_batch_size = 64

# Dirichlet Partitioning (matching HierFL for fair comparison)
[tool.flwr.hierarchy.dirichlet]
alpha_server = 0.5    # Non-IID across servers (heterogeneous edge regions)
alpha_client = 1000.0 # IID within each server (high alpha = uniform split)
seed         = 42


# Cloud Clustering Configuration
[tool.flwr.cloud_cluster]
enable = true
start_round = 30                 # clustering begins after models differentiate
frequency = 1                    # cluster every cloud round
method = "weight"                # "weight" clustering (v3 approach)
tau = 0.4                        # Similarity threshold (v3 value)

# System Configuration
[tool.flwr.cluster.batch_sizes]
logit_batch = 256
feature_batch = 256

[tool.flwr.system]
max_workers = 4
log_level = "INFO"
verbose_logging = true
# Note: gc_frequency hardcoded in orchestrator (every 10 rounds) like HHAR
# Note: communication_timeout, max_retries removed (not needed for in-memory)
